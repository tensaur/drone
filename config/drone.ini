[base]
package = simulator.env
env_name = drone
policy_name = Policy
rnn_name = None
vec = native

[env]
num_envs = 1024

[train]
anneal_lr = false
batch_size = 262144
bptt_horizon = 2
checkpoint_interval = 50
clip_coef = 0.1
clip_vloss = true
compile = false
compile_mode = reduce-overhead
cpu_offload = false
data_dir = experiments
device = cpu
ent_coef = 0.02908699591475944
env_batch_size = 1
gae_lambda = 0.47348058390175896
gamma = 0.5067347189236248
learning_rate = 0.00139090119603925
max_grad_norm = 1.3820903887464009
minibatch_size = 4096
norm_adv = true
num_envs = 1
num_workers = 1
total_timesteps = 100000000
update_epochs = 3
vf_clip_coef = 0.1
vf_coef = 0.27095743477429224
zero_copy = true
seed = 16
torch_deterministic = True
target_kl = None


[sweep]
method = bayes
name = sweep

[sweep.metric]
goal = maximize
name = environment/episode_return

[sweep.parameters.train.parameters.learning_rate]
distribution = log_uniform_values
min = 1e-5
max = 1e-1

[sweep.parameters.train.parameters.gamma]
distribution = uniform
min = 0.0
max = 1.0

[sweep.parameters.train.parameters.gae_lambda]
distribution = uniform
min = 0.0
max = 1.0

[sweep.parameters.train.parameters.update_epochs]
distribution = int_uniform
min = 1
max = 4

[sweep.parameters.train.parameters.vf_coef]
distribution = uniform
min = 0.0
max = 1.0

[sweep.parameters.train.parameters.max_grad_norm]
distribution = uniform
min = 0.0
max = 10.0

[sweep.parameters.train.parameters.ent_coef]
distribution = log_uniform_values
min = 1e-5
max = 1e-1

[sweep.parameters.train.parameters.bptt_horizon]
values = [1, 2, 4, 8, 16]
