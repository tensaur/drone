[base]
package = simulator.env
env_name = drone
policy_name = Policy
rnn_name = None
; rnn_name = Recurrent
vec = native

[env]
num_envs = 1024

[train]
anneal_lr = false
batch_size = 120_000
bptt_horizon = 2
checkpoint_interval = 50
clip_coef = 0.1
clip_vloss = true
compile = false
compile_mode = reduce-overhead
cpu_offload = false
data_dir = experiments
device = cpu
ent_coef = 0.02
env_batch_size = 1
gae_lambda = 0.95
gamma = 0.99
learning_rate = 0.0003
max_grad_norm = 0.5
minibatch_size = 30_000
norm_adv = true
num_envs = 1
num_workers = 1
total_timesteps = 1_000_000_000
update_epochs = 1
vf_clip_coef = 0.1
vf_coef = 0.5
zero_copy = true
seed = 16
torch_deterministic = True
target_kl = None


[sweep]
method = bayes
name = sweep

[sweep.metric]
goal = maximize
name = environment/episode_return

[sweep.parameters.train.parameters.learning_rate]
distribution = log_uniform_values
min = 1e-5
max = 1e-1

[sweep.parameters.train.parameters.gamma]
distribution = uniform
min = 0.0
max = 1.0

[sweep.parameters.train.parameters.gae_lambda]
distribution = uniform
min = 0.0
max = 1.0

[sweep.parameters.train.parameters.max_grad_norm]
distribution = uniform
min = 0.0
max = 10.0

[sweep.parameters.train.parameters.ent_coef]
distribution = log_uniform_values
min = 1e-5
max = 1e-1

